{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gv2i9fNDvrg"
   },
   "source": [
    "## Part A: Subreddit Prediction ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG9BpbQt3-ko"
   },
   "outputs": [],
   "source": [
    "subreddit_train = \"coursework_subreddit_train.json\"\n",
    "subreddit_test = \"coursework_subreddit_test.json\"\n",
    "\n",
    "#!gsutil cp gs://textasdata/coursework/coursework_subreddit_train.json $subreddit_train \n",
    "#!gsutil cp gs://textasdata/coursework/coursework_subreddit_test.json  $subreddit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCEG8t6PC2f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_self_post', 'posts', 'subreddit', 'title', 'url']\n",
      "   is_self_post                                              posts  \\\n",
      "0           1.0  [{'body': 'I think everyone has that one frien...   \n",
      "1           1.0  [{'body': 'I not 100% sure this is the right p...   \n",
      "2           1.0  [{'body': '', 'author': 'Leisure321', 'url': '...   \n",
      "3           1.0  [{'body': 'It's called 'forgetting things'.', ...   \n",
      "4           1.0  [{'body': 'How would I do this? I am looking t...   \n",
      "\n",
      "        subreddit                                              title  \\\n",
      "0   relationships  How do I [23F] communicate with my self-center...   \n",
      "1  summonerschool  What Cherry switch do you recommend for League...   \n",
      "2       askreddit                   Where do memes go when they die?   \n",
      "3           trees                     Some weird long term affects??   \n",
      "4        buildapc  Simple question: If I install Windows to a sta...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.reddit.com/r/relationships/comment...  \n",
      "1  https://www.reddit.com/r/summonerschool/commen...  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/4d...  \n",
      "3  https://www.reddit.com/r/trees/comments/1h300m...  \n",
      "4  https://www.reddit.com/r/buildapc/comments/jhb...  \n",
      "7280\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_threads = pd.read_json(path_or_buf=subreddit_train, lines=True)\n",
    "print(list(train_threads.columns.values))\n",
    "print(train_threads.head())\n",
    "print(train_threads.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89UU3g27C8SZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_self_post                                              posts  \\\n",
      "0           1.0  [{'body': 'Was watching a VOD from last years ...   \n",
      "1           1.0  [{'body': 'Basically what the title says.', 'u...   \n",
      "2           1.0  [{'body': '', 'author': 'Daft-Punk', 'url': 'h...   \n",
      "3           1.0  [{'body': 'I start running this year. I do it ...   \n",
      "4           1.0  [{'body': '[deleted]', 'url': 'https://www.red...   \n",
      "\n",
      "       subreddit                                              title  \\\n",
      "0      starcraft  Just a reminder on how much SC2 has evolved th...   \n",
      "1    whowouldwin  Your Favorite Hero Now Has A Healing Factor As...   \n",
      "2      askreddit  If you could live anywhere in the world, where...   \n",
      "3      askreddit                   Do you ever get use to exercise?   \n",
      "4  tipofmytongue         [TOMT] [book] A scary french book for kids   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.reddit.com/r/starcraft/comments/mq...  \n",
      "1  https://www.reddit.com/r/whowouldwin/comments/...  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/27...  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/x9...  \n",
      "4  https://www.reddit.com/r/tipofmytongue/comment...  \n",
      "1825\n"
     ]
    }
   ],
   "source": [
    "test_threads = pd.read_json(path_or_buf=subreddit_test, lines=True)\n",
    "print(test_threads.head())\n",
    "print(test_threads.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Nl9qzazDQ_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     20.000000\n",
      "mean      72.800000\n",
      "std       73.368285\n",
      "min       28.000000\n",
      "25%       36.250000\n",
      "50%       45.500000\n",
      "75%       63.750000\n",
      "max      334.000000\n",
      "Name: subreddit, dtype: float64\n",
      "askreddit               334\n",
      "leagueoflegends         196\n",
      "buildapc                131\n",
      "explainlikeimfive        82\n",
      "trees                    66\n",
      "techsupport              63\n",
      "pcmasterrace             62\n",
      "gaming                   62\n",
      "electronic_cigarette     59\n",
      "relationships            48\n",
      "tipofmytongue            43\n",
      "hearthstone              38\n",
      "jailbreak                38\n",
      "summonerschool           37\n",
      "atheism                  37\n",
      "reddit.com               34\n",
      "whowouldwin              33\n",
      "movies                   33\n",
      "personalfinance          32\n",
      "starcraft                28\n",
      "Name: subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "subreddit_counts = train_threads['subreddit'].value_counts()\n",
    "print(subreddit_counts.describe())\n",
    "top_subbreddits = subreddit_counts.nlargest(20)\n",
    "top_subbreddits_list = top_subbreddits.index.tolist()\n",
    "print(top_subbreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02OXNfo9H8Kc"
   },
   "outputs": [],
   "source": [
    "train_labels = train_threads['subreddit']\n",
    "test_labels = test_threads['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.DependencyParser at 0x7f22042fda98>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise nlp\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load('/usr/lib/python3.7/site-packages/en_core_web_md/en_core_web_md-2.0.0', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/stuart/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and normalizer\n",
    "\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "    tokens = list()\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "    normalized_tokens = list()\n",
    "    for token in tokens:\n",
    "        normalized = token.text.lower().strip()\n",
    "        if ((token.is_alpha or token.is_digit)):\n",
    "          normalized_tokens.append(normalized)\n",
    "    return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "    return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizers\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "one_hot_vectorizer = CountVectorizer(tokenizer=tokenize_normalize, binary=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classifiers\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "most_freq_class = DummyClassifier(strategy=\"most_frequent\")\n",
    "rand_class = DummyClassifier(strategy=\"stratified\")\n",
    "log_reg_class = LogisticRegression(solver=\"saga\")\n",
    "svc_class = SVC(kernel=\"rbf\")\n",
    "ber_nb_class = BernoulliNB() # Chosen classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_posts(threads):\n",
    "    \"\"\" Lines below explained in order:\n",
    "    Split posts column into new columns\n",
    "    Merge the back into train_threads\n",
    "    Drop the unexpanded posts\n",
    "    Transform the numeric rows from the split into separate rows\n",
    "    Remove variable column left from melt\n",
    "    Remove all rows with NaN\n",
    "    \"\"\"\n",
    "    expanded_posts = train_threads.posts.apply(pd.Series)\\\n",
    "    .merge(train_threads.rename(columns={'url': 'thread_url'}), left_index=True, right_index=True)\\\n",
    "    .drop([\"posts\"], axis=1)\\\n",
    "    .melt(id_vars=[\"is_self_post\", \"subreddit\", \"title\", \"thread_url\"], value_name=\"post\")\\\n",
    "    .drop(\"variable\", axis=1)\\\n",
    "    .dropna()\n",
    "\n",
    "    # Expand the post dictionary into columns and remove invalid posts\n",
    "    return pd.concat([\n",
    "        expanded_posts.drop(['post'], axis=1),\n",
    "        expanded_posts['post'].apply(pd.Series)\n",
    "    ], axis=1)\\\n",
    ".dropna(subset=['body', 'author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure threads into posts\n",
    "train_posts = expand_posts(train_threads)\n",
    "test_posts = expand_posts(test_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def evaluation_summary(description, predictions, true_labels):\n",
    "    print(\"Evaluation for: \" + description)\n",
    "    precision = precision_score(predictions, true_labels)\n",
    "    recall = recall_score(predictions, true_labels)\n",
    "    accuracy = accuracy_score(predictions, true_labels)\n",
    "    f1 = fbeta_score(predictions, true_labels, 1) #1 means f_1 measure\n",
    "    print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
    "    print(classification_report(predictions, true_labels, digits=3))\n",
    "    print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "one_hot_pipeline = Pipeline([\n",
    "    (\"union\", FeatureUnion([\n",
    "        (\"title\", Pipeline([\n",
    "            (\"select\", ItemSelector('title')),\n",
    "            (\"vec\", one_hot_vectorizer)\n",
    "        ])),\n",
    "        (\"body\", Pipeline([\n",
    "            (\"select\", ItemSelector('body')),\n",
    "            (\"vec\", one_hot_vectorizer)\n",
    "        ])),\n",
    "        (\"author\", Pipeline([\n",
    "            (\"select\", ItemSelector('author')),\n",
    "            (\"vec\", one_hot_vectorizer)\n",
    "        ])),\n",
    "    ]))\n",
    "])\n",
    "train_one_hot_features = one_hot_pipeline.fit_transform(train_posts)\n",
    "test_one_hot_features = one_hot_pipeline.transform(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline([\n",
    "    (\"union\", FeatureUnion([\n",
    "        (\"title\", Pipeline([\n",
    "            (\"select\", ItemSelector('title')),\n",
    "            (\"vec\", tfidf_vectorizer)\n",
    "        ])),\n",
    "        (\"body\", Pipeline([\n",
    "            (\"select\", ItemSelector('body')),\n",
    "            (\"vec\", tfidf_vectorizer)\n",
    "        ])),\n",
    "        (\"author\", Pipeline([\n",
    "            (\"select\", ItemSelector('author')),\n",
    "            (\"vec\", tfidf_vectorizer)\n",
    "        ])),\n",
    "    ]))\n",
    "])\n",
    "train_tfidf_features = tfidf_pipeline.fit_transform(train_posts)\n",
    "test_tfidf_features = tfidf_pipeline.transform(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: One-Hot Most Freq\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c6b359ef7522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmost_freq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_one_hot_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluation_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One-Hot Most Freq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_freq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_one_hot_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-c4d4fe616683>\u001b[0m in \u001b[0;36mevaluation_summary\u001b[0;34m(description, predictions, true_labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluation_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation for: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m-> 1047\u001b[0;31m                              \"choose another average setting.\" % y_type)\n\u001b[0m\u001b[1;32m   1048\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting."
     ]
    }
   ],
   "source": [
    "most_freq_class.fit(train_one_hot_features, train_labels)\n",
    "evaluation_summary(\"One-Hot Most Freq\", most_freq_class.predict(test_one_hot_features), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot title features\n",
    "one_hot_vectorizer.fit(train_posts['title'])\n",
    "train_title_one_hot_features = one_hot_vectorizer.transform(train_posts['title'])\n",
    "test_title_one_hot_features = one_hot_vectorizer.transform(test_posts['title'])\n",
    "\n",
    "# OneHot body features\n",
    "one_hot_vectorizer.fit(train_posts['body'])\n",
    "train_body_one_hot_features = one_hot_vectorizer.transform(train_posts['body'])\n",
    "test_body_one_hot_features = one_hot_vectorizer.transform(test_posts['body'])\n",
    "\n",
    "# OneHot author features\n",
    "one_hot_vectorizer.fit(train_posts['author'])\n",
    "train_author_one_hot_features = one_hot_vectorizer.transform(train_posts['author'])\n",
    "test_author_one_hot_features = one_hot_vectorizer.transform(test_posts['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf title features\n",
    "tfidf_vectorizer.fit(test_posts['title'])\n",
    "train_title_tfidf_features = tfidf_vectorizer.transform(train_posts['title'])\n",
    "test_title_tfidf_features = tfidf_vectorizer.transform(test_posts['title'])\n",
    "\n",
    "# Tfidf body features\n",
    "tfidf_vectorizer.fit(train_posts['body'])\n",
    "train_body_tfidf_features = tfidf_vectorizer.transform(train_posts['body'])\n",
    "test_body_tfidf_features = tfidf_vectorizer.transform(test_posts['body'])\n",
    "\n",
    "# Tfidf author features\n",
    "tfidf_vectorizer.fit(train_posts['author'])\n",
    "train_author_tfidf_features = tfidf_vectorizer.transform(train_posts['author'])\n",
    "test_author_tfidf_features = tfidf_vectorizer.transform(test_posts['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-G938e8D5qn"
   },
   "source": [
    "## Part B: Discourse prediction ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KM6aJtSETPY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://textasdata/coursework/coursework_discourse_train.json...\n",
      "\\ [1 files][ 60.2 MiB/ 60.2 MiB]                                                \n",
      "Operation completed over 1 objects/60.2 MiB.                                     \n",
      "Copying gs://textasdata/coursework/coursework_discourse_test.json...\n",
      "| [1 files][ 15.1 MiB/ 15.1 MiB]                                                \n",
      "Operation completed over 1 objects/15.1 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "discourse_train = \"coursework_discourse_train.json\"\n",
    "discourse_test = \"coursework_discourse_test.json\"\n",
    "  \n",
    "!gsutil cp gs://textasdata/coursework/coursework_discourse_train.json $discourse_train  \n",
    "!gsutil cp gs://textasdata/coursework/coursework_discourse_test.json  $discourse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwOaf_6aD-Vh"
   },
   "outputs": [],
   "source": [
    "# The reddit thread structure is nested with posts in a new content.\n",
    "# This block reads the file as json and creates a new data frame.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_posts(file):\n",
    "  # A temporary variable to store the list of post content.\n",
    "  posts_tmp = list()\n",
    "\n",
    "  with open(file) as jsonfile:\n",
    "    for i, line in enumerate(jsonfile):\n",
    "     # if (i > 2): break\n",
    "      thread = json.loads(line)\n",
    "      for post in thread['posts']:\n",
    "        # NOTE: This could be changed to use additional features from the post or thread.\n",
    "        # DO NOT change the labels for the test set.\n",
    "        posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
    "                        post['id'], post.get('author', \"\"), post.get('body', \"\"), post.get(\"majority_link\", \"\"), \n",
    "                        post.get('post_depth', 0), post.get('majority_type', \"\"), # discourse type label \n",
    "                        post.get('in_reply_to', \"\") ))\n",
    "\n",
    "# Create the posts data frame.  \n",
    "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body', 'majority_link', \n",
    "          'post_depth', 'discourse_type', 'in_reply_to']\n",
    "  return pd.DataFrame(posts_tmp, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDzHTDcmEQ11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    subreddit                           title  \\\n",
      "0  worldofpvp  Help me decide my new PvP main   \n",
      "1  worldofpvp  Help me decide my new PvP main   \n",
      "2  worldofpvp  Help me decide my new PvP main   \n",
      "3  worldofpvp  Help me decide my new PvP main   \n",
      "4  worldofpvp  Help me decide my new PvP main   \n",
      "\n",
      "                                                 url          id  \\\n",
      "0  https://www.reddit.com/r/worldofpvp/comments/2...   t3_2v0anq   \n",
      "1  https://www.reddit.com/r/worldofpvp/comments/2...  t1_codb2p9   \n",
      "2  https://www.reddit.com/r/worldofpvp/comments/2...  t1_codg0we   \n",
      "3  https://www.reddit.com/r/worldofpvp/comments/2...  t1_coeatsq   \n",
      "4  https://www.reddit.com/r/worldofpvp/comments/2...  t1_codbyit   \n",
      "\n",
      "         author                                               body  \\\n",
      "0      TyrickEU  Hi. \\nAs a raider previously, i had no problem...   \n",
      "1          vurt  [deleted]  \\n ^^^^^^^^^^^^^^^^0.5422 \\n > [Wha...   \n",
      "2   OptimusNice  This goes mostly for 3v3 since that seems to b...   \n",
      "3                Rets are in a good position right now, althoug...   \n",
      "4  Rageinjector  Druid are the best pvp healer atm and are grea...   \n",
      "\n",
      "  majority_link  post_depth discourse_type in_reply_to  \n",
      "0          none           0       question              \n",
      "1     t3_2v0anq           1         answer   t3_2v0anq  \n",
      "2     t3_2v0anq           1         answer   t3_2v0anq  \n",
      "3     t3_2v0anq           1         answer   t3_2v0anq  \n",
      "4     t3_2v0anq           1         answer   t3_2v0anq  \n",
      "Num posts:  792670\n"
     ]
    }
   ],
   "source": [
    "train_posts = load_posts(discourse_train)\n",
    "# Filter out empty labels\n",
    "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
    "print(train_posts.head())\n",
    "print(\"Num posts: \", train_posts.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqGLzyTOGadY"
   },
   "source": [
    "The label for the post we will be predicting is in the discourse_type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vvo7hfCEmvj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num posts:  198120\n"
     ]
    }
   ],
   "source": [
    "test_posts = load_posts(discourse_test)\n",
    "# Filter out empty labels\n",
    "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
    "print(\"Num posts: \", test_posts.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jat55HhNHGBp"
   },
   "outputs": [],
   "source": [
    "train_labels = train_posts['discourse_type']\n",
    "test_labels = test_posts['discourse_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFl6sM58HNFp"
   },
   "source": [
    "Examine the distribution over labels on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3NbLPBhFOkp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       10.000000\n",
      "mean      7926.700000\n",
      "std       9664.321866\n",
      "min       1266.000000\n",
      "25%       1671.500000\n",
      "50%       3235.500000\n",
      "75%      11919.750000\n",
      "max      31419.000000\n",
      "Name: discourse_type, dtype: float64\n",
      "answer              31419\n",
      "elaboration         14775\n",
      "question            13610\n",
      "appreciation         6849\n",
      "agreement            3868\n",
      "disagreement         2603\n",
      "humor                1787\n",
      "other                1633\n",
      "announcement         1457\n",
      "negativereaction     1266\n",
      "Name: discourse_type, dtype: int64\n",
      "['answer', 'elaboration', 'question', 'appreciation', 'agreement', 'disagreement', 'humor', 'other', 'announcement', 'negativereaction']\n"
     ]
    }
   ],
   "source": [
    "discourse_counts = train_labels.value_counts()\n",
    "print(discourse_counts.describe())\n",
    "\n",
    "top_discourse = discourse_counts.nlargest(200)\n",
    "print(top_discourse)\n",
    "top_discourse = top_discourse.index.tolist()\n",
    "print(top_discourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pbVa1fkZGHVQ"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-916b3084450d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-916b3084450d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    X_train = #features from training data\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define the features\n",
    "X_train = #features from training data\n",
    "X_test = #features from test data\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='stratified',random_state=0)\n",
    "clf.fit(X_train, train_posts['discourse_type'])\n",
    "predictions = clf.predict(X_test)  \n",
    "print(classification_report(predictions, test_posts['discourse_type']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Coursework skeleton.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
